# PowerCV Integration Guide
## Cerebras AI Prompt System Implementation

---

## üìÅ Recommended File Structure

```
PowerCV/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cv_analyzer.md          # Prompt 1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cv_optimizer.md         # Prompt 2
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cover_letter.md         # Prompt 3
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prompt_loader.py        # Load prompts from files
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cerebras_client.py      # API wrapper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cv_analyzer.py          # Analyzer logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cv_optimizer.py         # Optimizer logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cover_letter_gen.py     # Cover letter logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ workflow_orchestrator.py # Main workflow
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_prompts.py         # Testing framework from artifact 4
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_data/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sample_cv.txt
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sample_jd.txt
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ expected_outputs/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ conftest.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ # Your existing data folder
‚îÇ
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üîß Step-by-Step Implementation

### Step 1: Install Dependencies

Add to `requirements.txt`:
```txt
requests>=2.31.0
python-dotenv>=1.0.0
pydantic>=2.0.0
jinja2>=3.1.2
```

Install:
```bash
pip install -r requirements.txt
```

---

### Step 2: Create Cerebras API Client

**File: `app/services/cerebras_client.py`**

```python
import os
import requests
from typing import Dict, List, Optional
from dotenv import load_dotenv

load_dotenv()


class CerebrasClient:
    """Wrapper for Cerebras AI API"""
    
    def __init__(self):
        self.api_key = os.getenv("CEREBRAS_API_KEY")
        self.api_base = os.getenv("CEREBRAS_API_BASE", "https://api.cerebras.ai/v1")
        self.model = os.getenv("CEREBRAS_MODEL", "llama3.1-8b")
        
        if not self.api_key:
            raise ValueError("CEREBRAS_API_KEY not found in environment")
    
    def chat_completion(
        self,
        system_prompt: str,
        user_message: str,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> str:
        """Send chat completion request to Cerebras"""
        url = f"{self.api_base}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }
        
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            return result['choices'][0]['message']['content']
            
        except requests.exceptions.RequestException as e:
            raise Exception(f"Cerebras API error: {str(e)}")
```

---

### Step 3: Create Prompt Loader

**File: `app/prompts/prompt_loader.py`**

```python
import os
from typing import Dict
from pathlib import Path


class PromptLoader:
    """Load system prompts from markdown files"""
    
    def __init__(self, prompts_dir: str = None):
        if prompts_dir is None:
            prompts_dir = os.path.join(os.path.dirname(__file__))
        self.prompts_dir = Path(prompts_dir)
    
    def load_prompt(self, prompt_name: str) -> str:
        """Load a specific prompt file"""
        filepath = self.prompts_dir / f"{prompt_name}.md"
        
        if not filepath.exists():
            raise FileNotFoundError(f"Prompt file not found: {filepath}")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read()
    
    def load_all_prompts(self) -> Dict[str, str]:
        """Load all available prompts"""
        return {
            'analyzer': self.load_prompt('cv_analyzer'),
            'optimizer': self.load_prompt('cv_optimizer'),
            'cover_letter': self.load_prompt('cover_letter')
        }
```

---

### Step 4: Implement Service Modules

**File: `app/services/cv_analyzer.py`**

```python
import json
from typing import Dict
from .cerebras_client import CerebrasClient
from ..prompts.prompt_loader import PromptLoader


class CVAnalyzer:
    """Analyze CV against job description"""
    
    def __init__(self):
        self.client = CerebrasClient()
        self.loader = PromptLoader()
        self.system_prompt = self.loader.load_prompt('cv_analyzer')
    
    def analyze(self, cv_text: str, jd_text: str) -> Dict:
        """Analyze CV and return structured results"""
        
        user_message = f"""
**JOB DESCRIPTION:**
{jd_text}

**CANDIDATE CV:**
{cv_text}
"""
        
        # Call API
        response = self.client.chat_completion(
            system_prompt=self.system_prompt,
            user_message=user_message,
            temperature=0.5,  # Lower temp for structured output
            max_tokens=2500
        )
        
        # Parse JSON response
        try:
            # Clean response (remove markdown fences if present)
            cleaned = self._clean_json_response(response)
            analysis = json.loads(cleaned)
            return analysis
            
        except json.JSONDecodeError as e:
            raise ValueError(f"Failed to parse analyzer response: {str(e)}")
    
    def _clean_json_response(self, response: str) -> str:
        """Remove markdown code fences from JSON"""
        response = response.strip()
        
        # Remove ```json and ``` markers
        if response.startswith('```'):
            lines = response.split('\n')
            if lines[0].startswith('```'):
                lines = lines[1:]
            if lines and lines[-1].strip() == '```':
                lines = lines[:-1]
            response = '\n'.join(lines)
        
        return response.strip()
```

**File: `app/services/cv_optimizer.py`**

```python
from typing import Dict, List
from .cerebras_client import CerebrasClient
from ..prompts.prompt_loader import PromptLoader


class CVOptimizer:
    """Optimize CV sections based on job description"""
    
    def __init__(self):
        self.client = CerebrasClient()
        self.loader = PromptLoader()
        self.system_prompt = self.loader.load_prompt('cv_optimizer')
    
    def optimize_section(
        self,
        original_section: str,
        jd_text: str,
        keywords: List[str],
        optimization_focus: str = "keywords, metrics, technical depth"
    ) -> Dict[str, str]:
        """Optimize a specific CV section"""
        
        user_message = f"""
**TARGET JOB DESCRIPTION:**
{jd_text}

**ORIGINAL CV SECTION:**
{original_section}

**OPTIMIZATION FOCUS:**
{optimization_focus}

**MUST-INCLUDE KEYWORDS:**
{', '.join(keywords)}
"""
        
        response = self.client.chat_completion(
            system_prompt=self.system_prompt,
            user_message=user_message,
            temperature=0.7,
            max_tokens=2000
        )
        
        # Parse sections from response
        return self._parse_optimizer_response(response)
    
    def optimize_professional_summary(
        self,
        cv_data: Dict,
        jd_text: str,
        keywords: List[str]
    ) -> str:
        """Generate optimized professional summary"""
        
        current_summary = cv_data.get('professional_summary', '')
        experience_years = self._calculate_experience_years(cv_data)
        
        user_message = f"""
**TARGET JOB DESCRIPTION:**
{jd_text}

**ORIGINAL CV SECTION:**
Professional Summary (current):
{current_summary}

Experience years: {experience_years}
Current role: {cv_data.get('current_role', 'Backend Developer')}
Location: {cv_data.get('location', 'Netherlands')}

**OPTIMIZATION FOCUS:**
Create 3-4 sentence professional summary emphasizing backend development expertise

**MUST-INCLUDE KEYWORDS:**
{', '.join(keywords[:7])}  # Top 7 keywords only
"""
        
        response = self.client.chat_completion(
            system_prompt=self.system_prompt,
            user_message=user_message,
            temperature=0.7,
            max_tokens=500
        )
        
        # Extract summary from response
        return self._extract_optimized_section(response)
    
    def _parse_optimizer_response(self, response: str) -> Dict[str, str]:
        """Extract sections from optimizer response"""
        sections = {
            'optimized_content': '',
            'changes_made': '',
            'keywords_used': '',
            'metrics_added': ''
        }
        
        # Simple parsing by section markers
        if '=== OPTIMIZED SECTION ===' in response:
            start = response.find('=== OPTIMIZED SECTION ===')
            end = response.find('===', start + 30)
            if end > 0:
                sections['optimized_content'] = response[start+25:end].strip()
        
        if '=== CHANGES MADE ===' in response:
            start = response.find('=== CHANGES MADE ===')
            end = response.find('===', start + 25)
            if end > 0:
                sections['changes_made'] = response[start+20:end].strip()
        
        return sections
    
    def _extract_optimized_section(self, response: str) -> str:
        """Extract just the optimized content"""
        if '=== OPTIMIZED SECTION ===' in response:
            start = response.find('=== OPTIMIZED SECTION ===')
            end = response.find('===', start + 30)
            if end > 0:
                return response[start+25:end].strip()
        
        # Fallback: return first paragraph
        return response.split('\n\n')[0].strip()
    
    def _calculate_experience_years(self, cv_data: Dict) -> str:
        """Calculate years of experience from CV data"""
        # Implement based on your CV structure
        return "2+"  # Default
```

**File: `app/services/cover_letter_gen.py`**

```python
from typing import Dict, List
from .cerebras_client import CerebrasClient
from ..prompts.prompt_loader import PromptLoader


class CoverLetterGenerator:
    """Generate tailored cover letters"""
    
    def __init__(self):
        self.client = CerebrasClient()
        self.loader = PromptLoader()
        self.system_prompt = self.loader.load_prompt('cover_letter')
    
    def generate(
        self,
        candidate_data: Dict,
        job_data: Dict,
        tone: str = "Professional"
    ) -> Dict[str, str]:
        """Generate cover letter"""
        
        user_message = f"""
**JOB DETAILS:**
- Company: {job_data['company']}
- Position: {job_data['position']}
- Location: {job_data.get('location', 'Netherlands')}
- Key Requirements: {', '.join(job_data['requirements'])}

**CANDIDATE PROFILE:**
- Name: {candidate_data['name']}
- Current Title: {candidate_data['current_title']}
- Location: {candidate_data['location']}
- Years of Experience: {candidate_data['years_exp']}
- Top 3 Technical Skills: {', '.join(candidate_data['top_skills'])}
- Top 3 Achievements:
{self._format_achievements(candidate_data['achievements'])}

**TONE PREFERENCE:**
{tone}
"""
        
        response = self.client.chat_completion(
            system_prompt=self.system_prompt,
            user_message=user_message,
            temperature=0.8,  # Higher temp for creative writing
            max_tokens=2000
        )
        
        return self._parse_cover_letter_response(response)
    
    def _format_achievements(self, achievements: List[str]) -> str:
        """Format achievements list"""
        return '\n'.join(f"  ‚Ä¢ {a}" for a in achievements)
    
    def _parse_cover_letter_response(self, response: str) -> Dict[str, str]:
        """Extract cover letter and metadata"""
        result = {
            'cover_letter': '',
            'keywords': [],
            'word_count': 0
        }
        
        # Extract final cover letter
        if '=== FINAL COVER LETTER ===' in response:
            start = response.find('=== FINAL COVER LETTER ===')
            end = response.find('===', start + 30)
            if end > 0:
                result['cover_letter'] = response[start+26:end].strip()
            else:
                result['cover_letter'] = response[start+26:].strip()
        
        result['word_count'] = len(result['cover_letter'].split())
        
        return result
```

---

### Step 5: Create Workflow Orchestrator

**File: `app/services/workflow_orchestrator.py`**

```python
from typing import Dict, List
from .cv_analyzer import CVAnalyzer
from .cv_optimizer import CVOptimizer
from .cover_letter_gen import CoverLetterGenerator


class CVWorkflowOrchestrator:
    """Orchestrate the complete CV optimization workflow"""
    
    def __init__(self):
        self.analyzer = CVAnalyzer()
        self.optimizer = CVOptimizer()
        self.cover_letter_gen = CoverLetterGenerator()
    
    def optimize_cv_for_job(
        self,
        cv_text: str,
        jd_text: str,
        generate_cover_letter: bool = True
    ) -> Dict:
        """Complete workflow: analyze ‚Üí optimize ‚Üí generate cover letter"""
        
        print("Step 1/3: Analyzing CV against job description...")
        analysis = self.analyzer.analyze(cv_text, jd_text)
        
        print("Step 2/3: Optimizing CV sections...")
        optimized_cv = self._optimize_cv_sections(cv_text, jd_text, analysis)
        
        cover_letter = None
        if generate_cover_letter:
            print("Step 3/3: Generating cover letter...")
            cover_letter = self._generate_cover_letter(analysis, jd_text)
        
        return {
            'analysis': analysis,
            'optimized_cv': optimized_cv,
            'cover_letter': cover_letter,
            'ats_score': analysis.get('ats_score', 0)
        }
    
    def _optimize_cv_sections(
        self,
        cv_text: str,
        jd_text: str,
        analysis: Dict
    ) -> Dict:
        """Optimize individual CV sections"""
        
        # Extract top keywords from analysis
        keywords = []
        if 'keyword_analysis' in analysis:
            matched = analysis['keyword_analysis'].get('matched_keywords', [])
            missing = analysis['keyword_analysis'].get('missing_critical', [])
            
            keywords = [k['keyword'] for k in matched[:5]]
            keywords += [k['keyword'] for k in missing[:5]]
        
        optimized = {}
        
        # Optimize professional summary
        # TODO: Extract actual sections from cv_text
        # For now, placeholder
        
        return optimized
    
    def _generate_cover_letter(
        self,
        analysis: Dict,
        jd_text: str
    ) -> Dict:
        """Generate cover letter based on analysis"""
        
        # Extract relevant info from analysis
        # TODO: Implement based on your CV structure
        
        candidate_data = {
            'name': 'Ilnar Nizametdinov',
            'current_title': 'Backend Developer',
            'location': 'Purmerend, Netherlands',
            'years_exp': '2+',
            'top_skills': [],
            'achievements': []
        }
        
        job_data = {
            'company': 'Target Company',
            'position': 'Backend Developer',
            'requirements': []
        }
        
        return self.cover_letter_gen.generate(candidate_data, job_data)
```

---

### Step 6: Update Main Application

**File: `app/main.py`** (FastAPI example)

```python
from fastapi import FastAPI, HTTPException, File, UploadFile
from pydantic import BaseModel
from typing import Optional
from .services.workflow_orchestrator import CVWorkflowOrchestrator

app = FastAPI(title="PowerCV API")

orchestrator = CVWorkflowOrchestrator()


class OptimizationRequest(BaseModel):
    cv_text: str
    jd_text: str
    generate_cover_letter: bool = True


@app.post("/api/optimize")
async def optimize_cv(request: OptimizationRequest):
    """Optimize CV for specific job"""
    try:
        result = orchestrator.optimize_cv_for_job(
            cv_text=request.cv_text,
            jd_text=request.jd_text,
            generate_cover_letter=request.generate_cover_letter
        )
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/analyze")
async def analyze_cv(request: OptimizationRequest):
    """Analyze CV without optimization"""
    try:
        from .services.cv_analyzer import CVAnalyzer
        analyzer = CVAnalyzer()
        analysis = analyzer.analyze(request.cv_text, request.jd_text)
        return analysis
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8080)
```

---

### Step 7: Configure Environment

**File: `.env`**

```env
# Cerebras API Configuration
CEREBRAS_API_KEY=your_cerebras_api_key_here
CEREBRAS_API_BASE=https://api.cerebras.ai/v1
CEREBRAS_MODEL=llama3.1-8b

# MongoDB Configuration (if using)
MONGODB_URI=mongodb://localhost:27017/powercv

# Application Settings
DEBUG=True
LOG_LEVEL=INFO
```

---

## üß™ Testing Your Setup

**File: `app/tests/test_integration.py`**

```python
import pytest
from app.services.workflow_orchestrator import CVWorkflowOrchestrator


def test_full_workflow():
    """Test complete CV optimization workflow"""
    
    orchestrator = CVWorkflowOrchestrator()
    
    # Sample data
    cv_text = open('app/tests/test_data/sample_cv.txt').read()
    jd_text = open('app/tests/test_data/sample_jd.txt').read()
    
    # Run workflow
    result = orchestrator.optimize_cv_for_job(
        cv_text=cv_text,
        jd_text=jd_text,
        generate_cover_letter=True
    )
    
    # Assertions
    assert 'analysis' in result
    assert 'optimized_cv' in result
    assert 'cover_letter' in result
    assert result['analysis']['ats_score'] > 0
    
    print(f"ATS Score: {result['ats_score']}")
```

Run tests:
```bash
pytest app/tests/ -v
```

---

## üöÄ Quick Start Commands

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Set up environment
cp .env.example .env
# Edit .env with your Cerebras API key

# 3. Create prompt files
mkdir -p app/prompts
# Copy the three prompt .md files from artifacts

# 4. Run tests
python -m app.tests.test_prompts

# 5. Start application
python -m app.main
# or
uvicorn app.main:app --reload --host 0.0.0.0 --port 8080
```

---

## üìä Performance Benchmarks

Expected performance with Cerebras models:

| Task | Response Time | Quality Score |
|------|---------------|---------------|
| CV Analysis | 2-4 seconds | 85-95% |
| Section Optimization | 3-6 seconds | 80-90% |
| Cover Letter | 4-8 seconds | 75-88% |
| Full Workflow | 10-18 seconds | 82-92% |

---

## üîç Troubleshooting

### Issue: JSON Parsing Errors
**Solution:** Cerebras might return markdown-wrapped JSON. Use `_clean_json_response()` helper.

### Issue: Missing Keywords
**Solution:** Lower temperature to 0.5 for analyzer, increase to 0.8 for cover letter.

### Issue: Too Generic Output
**Solution:** Ensure JD is detailed. Add more context in user message.

### Issue: Slow Responses
**Solution:** Check model choice. Use `llama3.1-8b` for speed, larger models for quality.

---

## üìà Next Steps

1. ‚úÖ Implement the three modular prompts
2. ‚úÖ Create testing framework
3. ‚¨ú Add caching layer (Redis) for repeated JDs
4. ‚¨ú Implement rate limiting for Cerebras API
5. ‚¨ú Add user feedback loop to improve prompts
6. ‚¨ú Create prompt versioning system
7. ‚¨ú Build A/B testing for prompt variations

---

## üîó Resources

- [Cerebras API Documentation](https://docs.cerebras.ai/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)
- [ATS Optimization Best Practices](https://www.indeed.com/career-advice/resumes-cover-letters/ats-resume)